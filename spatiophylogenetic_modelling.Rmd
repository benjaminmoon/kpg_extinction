---
title: "Spatiophylogenetic modelling: the end-Cretaceous extinction"
bibliography: references.bib
---

# Introduction

The aim of this part of the project is to use occurrence data of bivalves from across the Cretaceousâ€“Palaeogene boundary to reconstruct extinction prevalence and potential drivers of this.

## Project environment ##


```{r renv_init, eval = FALSE}
renv::restore()
```

*renv*, used above, sets up a *project environment* that stores the packages and versions used to make the project work. The idea of this is to enable you to go back to this after some time and get the project working again, no matter what's changed or been updated in the meantime. Similarly, send the project to a collaborator and they can set things up to be working quickly and easily without having to search through packages to install. I've already *initialized* this project, to get things going on your computer should a matter of opening R in this filter. *renv* will then install itself and any packages needed into its own library.

```{r packages}
library(jsonlite)
library(furrr)
library(ggthemes)
library(mapproj)
library(rnaturalearth)
library(sf)
library(tidyverse)
library(viridis)
```

## Useful background and sites ##

I'm rather opinionated about code and formatting. I think it's generally better to be strict about the packages you use and how you write the code. I tend to use *tidyverse* style and formatting, which has [a guide](https://r4ds.had.co.nz/index.html) written by Hadley Wickham, who's created some of the most used R packages. Google have a [style guide](https://google.github.io/styleguide/Rguide.html) for writing R code that I recommend following, but as a guide can and should be changed to meet your needs.

I use *piping* a lot. This means passing the result of one function as the first argument of the next line. R has a native version shown as `|>`, but this is rather new (version 4.1.0). *tidyverse* and package *magrittr* has had a different version for while that looks like `%>%` and is probably the better to use, so you might see that online more often.

This guide is written in [R Markdown](https://rmarkdown.rstudio.com/articles_intro.html), a text format that combines text and code and can be run easily in R Studio and other environments. I also tend to use *ggplot* for plotting, so have look [here](https://ggplot2-book.org) for the low-down on that package.


# Acquiring data

The data come from the [Paleobiology Database](https://paleobiodb.org/#/) (PBDB). Occurrences are downloaded though their [API](https://paleobiodb.org/data1.2/).

First we can download the data as a JSON file and save this to disc: this is our master data set. For this, I've gathered occurrences of Bivalvia from the Cretaceous and Paleogene with all of the occurrence, geographical, systematic, and ecological data.

```{r download_pbdb_data, eval = FALSE}
bivalve_pbdb_url <-
  paste0(
    "https://paleobiodb.org/data1.2/", # PBDB API URL
    "occs/", # occurrences
    "list.json?", # download a JSON file
    "base_name=Bivalvia&", # download bivalve occurrences
    "interval=Cretaceous,Paleogene&", # temporal range to include
    "show=full" # include full data
  )
bivalve_pbdb_file <- paste0(Sys.Date(), "-bivalve_pbdb_occurrences.json")
download.file(bivalve_pbdb_url, bivalve_pbdb_file, method = "curl")
```

The data file can then be loaded into R, which gives a list with length two: the first element (`$elapsed_time`) gives the original download time and the second element (`$records`) is a `data.frame` of the PBDB occurrence data. We keep just this second part. To save space, I've downloaded a version and stored this as a compressed `tar` file that needs to be expanded first with `untar`.

```{r read_pbdb_data}
bivalve_pbdb_file <- "2022-03-18-bivalve_pbdb_occurrences"
untar(paste0(bivalve_pbdb_file, ".tgz"))
bivalve_pbdb_data <- jsonlite::fromJSON(paste0(bivalve_pbdb_file, ".json"))$records %>%
  as_tibble()
```

We won't need data from all the columns, but the important few are:

* `oid`: occurrence ID, a unique reference number.
* `idn`: the originally identified name.
* `tna`: the currently accepted name.
* `rnk`: taxonomic rank of the accepted name.
* `oei`, `oli`: earliest and latest occurrence interval names.
  - `eag` and `lag` are ages at the beginning and end of these intervals, taken from the [*Geologic Time Scale*](https://timescalecreator.org/index/index.php), but may want to update to a recent version.
* `fml`: family of the occurring taxon.
  - higher taxon levels are also included.
* `lng`, `lat`: modern longitude and latitude of the occurrence.
  - `pln` and `pln` are the reconstructed palaeocoordinates, but it may be worth re-doing these to confirm or with a different tectonic model.
* `env`: the palaeoenvironment, useful to check localities and habitats, exclude freshwater species etc. 
* Various [palaeocological/ecospace data](https://paleobiodb.org/data1.2/general/ecotaph_doc.html#Ecospace), such as habitat depth (`jlh`), general environment (`jev`), feeding mode (`jdt`).
* `jco`: composition/mineralogy of shells.

A full list of the fields is at the bottom of [this page](https://paleobiodb.org/data1.2/occs/list_doc.html#response).

# Querying data #

It's a good check to have a quick look at the data to check its size and what different values are contained within. Most of the PBDB data are categorical or strings, which makes it a little more difficult (you can't plot a bar chart for instance, except you can). A few things to check:

* How many records are identified to species level? genus? family?
* How many different species/genera/families are include?
* How many records don't have accepted values?

```{r check_species}
bivalve_pbdb_data %>%
  filter(rnk == 3) %>%
  group_by(tna) %>%
  summarize(n())
```

Or a more fun thing to plot the occurrences from different families.

```{r plot_family_occurrences}
family_occurrences <-
  bivalve_pbdb_data %>%
    filter(rnk == c(3, 4, 5)) %>%
    group_by(fml) %>%
    summarize(family_occurrences = n())
family_occurrences %>%
  ggplot(aes(x = reorder(fml, desc(family_occurrences)), y = family_occurrences)) +
    geom_col()
family_occurrences %>%
  slice_max(family_occurrences, n = 10)
```

## Data quality ##

Now we can do a more thorough look through the data to check that it's correct, or at least in the form we expect it to be. First is the relatively easy one: checking for numbers in columns that should have the, such as rnk, lng, and lat. On import this column is started as a character string in the tibble. We can check whether any of the records are not proper numbers by searching for characters not found in a number: i.e. not digits, decimal points, or minus signs.

```{r data_cleaning}
# search for strings that don't contain a digit, full stop/decimal point, or hyphen/minus, i.e. those that aren't a number
number_as_character_regexp <- "[^[[:digit:]]\\.\\-]"

check_numeric_column <- function(column, data) {
  column_vector <- pull(data, column)
  string_match <- str_which(column_vector, number_as_character_regexp)
  if (length(string_match) == 0) {
    print(paste(column, ": All items are (probably) numbers."))
  } else {
    print(paste(column, ": Some items (probably) aren't numbers."))
    string_match
  }
}

c("lng", "lat", "rnk") %>%
  walk(check_numeric_column, data = bivalve_pbdb_data)
```

Assuming this above comes out tickety-boo, then we can simply replace and reformat those columns to numeric values. It's always a good plan to create a new object doing this so that you don't lose the original data, just in case things go wrong later.

```{r reformatting_numeric_data}
bivalve_cleaned <-
  bivalve_pbdb_data %>%
  filter(str_detect(jev, "marine|shelf|oceanic")) %>%
  mutate(across(c(rnk, lng, lat), as.numeric))
```

The same should be done for the columns we expect to have character strings in, i.e. the accepted name and family column.


# Mapping data #

We can also do more fun things like plot the occurrences on a map. Useful guides to this are found in three blog posts [starting here](https://r-spatial.org//r/2018/10/25/ggplot2-sf.html). Working with mapping data is more involved than simple points, particularly if you want to change the projection away from the standard Mercator. The package `sf` (short for 'simple features', describing how the data is stored) can deal with all of this and then in plotting the projection can be changed. A rule of thumb is make sure to assign the coordinate reference system (CRS) when creating map objects.

Package `rnaturalearth` has regularly updated maps that should be better to use than in the built-in `maps` library. Here we download the country outlines in *sf* format.

```{r map_occurrences}
world_map <-
  ne_countries(returnclass = "sf")
```

Next, convert the occurrences locations into an *sf* object also. I've filtered down to occurrences identified to species level and then taken a random sample ('slice') of 1000 rows. The final stage is convert to an *sf* object using the longitude and latitude columns, with the standard WGS84 CRS (indicated with `4326` in this case). You can find this code to use looking at [epsg.io](https://epsg.io/4326), searching for the projection, and using the code given. In the case of WGS84 coordinates (standard latitude and longitude) shown this is 4326.

```{r get_occurrence_points}
occurrences_to_plot <-
  bivalve_cleaned %>%
  filter(rnk == 3) %>%
  slice_sample(n = 1000) %>%
  st_as_sf(coords = c("lng", "lat"), crs = 4326, agr = "constant")
```

Now, we put these together into the plot using the `geom_sf` function to build up the layers: map outline first, then points over the top. The colour scale is good for colour-blind people, so is a good option to prefer. Here I modify the map projection using `coord_sf(crs)` into the Mollweide projection. I need to work out about adding a border and grid lines at this point.

```{r plot_occurrence_map}
ggplot() +
  geom_sf(data = world_map, inherit.aes = FALSE) +
  geom_sf(data = occurrences_to_plot, mapping = aes(colour = fml)) +
  scale_colour_viridis(discrete = TRUE) +
  coord_sf(crs = "+proj=moll") +
  theme_bw() +
  theme(
    legend.position = "none",
    panel.border = element_blank()
  )
```

# Things to do #

* Look at the quality of the data: how many families? which ones are now extinct?
* Can you identify occurrences that have erroneous names? 
* What species go extinct across the Cretaceousâ€“Palaeogene boundary?
* Have a look at [this GitHub repository](https://github.com/benjaminmoon/palaeomap_example) showing how to get data and plot palaeogeographical maps. Can you do the same for occurrences in the Cretaceous and Palaeogene.

## Palaeogeographical map ##

We use the palaeogeographical reconstructions from the combined Muller2019-Young2019-Cao2020 (MYC) model provided by GPlates (<https://www.gplates.org>) [@Muller2019T; @Young2019GF; @Cao2020GR; @Torsvik2019GGG]. This provides a nice visualization of the palaeogeography including locations of shallow marine environments, land, mountains, and ice caps. We exported these four layers reconstructed to 68 Ma in the latest Cretaceous and plot them as a base map with modern coastlines indicated. The data files for the reconstructions are provided with GPlates or can be downloaded from [Earthbyte](http://www.earthbyte.org/gplates-2-3-software-and-data-sets/).

```{r fig.cap = "Palaeogeographical reconstruction in the latest Cretaceous (68 Ma), with modern coastlines indicated, based on the environmental reonctrution of @Cao2017Bb and the combined rotaion models of @Muller2019T, @Young2019GF, and @Cao2020GR, with correction to the Pacific by @Torsvik2019GGG."}
palaeogeo_file_layers <-
  c("Shallow marine" = "sm", "Landmass" = "lm", "Mountain" = "m", "Ice cap" = "i")

read_geojson <-
  function(prefix, filename = "./data/palaeogeographical_reconstructions/id_402_2_reconstructed_68.00Ma.geojson") {
  # Read a list of GeoJSON files and combine into a single sf collection.
  #
  # Arguments:
  #   prefix: layer prefixes for the data files ("sm", "lm", "m", "i") taken from Cao et al. (2017).
  #   filename: rest of GeoJSON file name.
  #
  # Returns:
  #   A single feature collection with geometries and ID column ("id").
  file <- str_replace(filename, "id", prefix)
  st_read(file) %>%
    add_column(id = prefix)
}

map_data <- palaeogeo_file_layers %>%
  map(read_geojson) %>%
  do.call(rbind, .) %>%
  mutate(
    id = factor(id, levels = palaeogeo_file_layers, labels = names(palaeogeo_file_layers))
  )

base_map <-
  ggplot() +
    geom_sf(data = map_data, aes(fill = id), colour = NA) +
    scale_discrete_manual(
      # colours for base map layers
      values =
        c(
          "Ice cap"        = "#DAD3FF",
          "Landmass"       = "#FFD23A",
          "Mountain"       = "#FF8D51",
          "Shallow marine" = "#45D8FF"
        ),
      aesthetics = c("fill"),
      name = "Palaeogeography"
    ) +
    theme_bw() +
    theme(panel.border = element_blank())

modern_coastlines <-
  st_read("./data/palaeogeographical_reconstructions/Global_EarthByte_GPlates_PresentDay_Coastlines_Polyline_reconstructed_68.00Ma.geojson")

base_map +
  geom_sf(data = modern_coastlines, colour = "grey60") +
  coord_sf(crs = "+proj=robin")
```

## Rotation modern locality coordinates

We have the modern localities for our bivalve occurrences, but need to rotate these to the position they were in at the end of the Cretaceous (68 Ma) to get their palaeocoordinates. The GPlates Web Service can do this online or through docker, but doesn't offer the rotation model used for the combined MYC palaeogeographical reconstruction, and can be quite slow to do many requestsâ€‰â€”â€‰and we have tens-of-thousands of bivalve occurrences. Instead, we field this out to [pyGPlates](https://www.gplates.org/docs/pygplates/index.html). This step of reconstruction has been done, so the file `./data/reconstructed_68Ma.gmt` can be read in directly.

```{r}
modern_coords <-
  bivalve_cleaned %>%
  filter(lag > 66 & lag < 75) %>%
  select(c("lng", "lat")) %>%
  st_as_sf(coords = c("lng", "lat"), crs = 4326, agr = "constant")

st_write(modern_coords, "./data/kpg_bivalve_modern_coordinates.gmt", driver = "OGR_GMT")
```

Instructions to install pyGPlates can be found [here](https://www.gplates.org/docs/pygplates/index.html), including the necessary Python packages (specifically `proj`). [Anaconda](https://www.anaconda.com)/[Miniconda](https://docs.conda.io/en/latest/miniconda.html) is a good Python managing system in which you can also create project environments. The code in the python blocks below can be run in a python environmentâ€‰â€”â€‰separate to the main R environmentâ€‰â€”â€‰or can be run directly within R using the *reticulate* package [@Ushey2022].

There are two steps to reconstructing the palaeocoordinates: (1) assigning the localities to the static polygons (i.e. plate fragments thay move with continental drift), and (2) rotating the plate fragments and localities to the palaeolocations. First we assign the points and output a GPML file that can be visualized in GPlates directly.

```{python assign_locality_polygons, eval = FALSE}
import pygplates

# Load one or more rotation files into a rotation model.
rotation_model = pygplates.RotationModel('./data/palaeogeographical_reconstructions/Muller2019-Young2019-Cao2020_CombinedRotations.rot')

# Filename for the static plate boundaries
static_polygons_filename = './data/palaeogeographical_reconstructions/Global_EarthByte_GPlates_PresentDay_StaticPlatePolygons.gpmlz'

# Load some features.
point_features = pygplates.FeatureCollection('./data/kpg_bivalve_modern_coordinates.gmt')

# Output filename for assigned points
assigned_points_output = './data/bivalve_assigned_locations.gpml'

# Place point occurrences onto plate static polygons
assigned_points = pygplates.partition_into_plates(
    static_polygons_filename,
    rotation_model,
    point_features,
    properties_to_copy = [
        pygplates.PartitionProperty.reconstruction_plate_id,
        pygplates.PartitionProperty.valid_time_period
    ]
)

# Write assigned points to a GPML file that can be used in GPlates
assigned_points_collection = pygplates.FeatureCollection(assigned_points)
assigned_points_collection.write(assigned_points_output)
```

Next we rotate the palaeolocations to 68Â Ma and export an OGR-GMT file of these palaeocoordinates.

```{python reconstruct_palaeocoordinates, eval = FALSE}
# Reconstruct features to this geological time.
reconstruction_time = 68

# The filename of the exported reconstructed geometries.
# It's a shapefile called 'reconstructed_68Ma.shp'.
export_filename = 'reconstructed_{0}Ma.gmt'.format(reconstruction_time)

# Reconstruct the features to the reconstruction time and export them to a shapefile.
pygplates.reconstruct(assigned_points_collection, rotation_model, export_filename, reconstruction_time)
```

These converted locations can be read back into R and plotted over the base map.

```{r read_palaeo_coords, fig.cap = "Palaeolocations of marine bivalve occurrences in the latest Cretaceous (70â€“66Â Ma) plotted on a palaeogeographical map at 68Â Ma.}
palaeo_coords <-
  st_read("./data/reconstructed_68Ma.gmt")

base_map +
  geom_sf(data = palaeo_coords) +
  coord_sf(crs = "+proj=robin")
```


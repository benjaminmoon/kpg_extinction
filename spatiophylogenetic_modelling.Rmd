---
title: "Spatiophylogenetic modelling: the end-Cretaceous extinction"
bibliography: references.bib
---

# Introduction

The aim of this part of the project is to use occurrence data of bivalves from across the Cretaceous–Palaeogene boundary to reconstruct extinction prevalence and potential drivers of this.

## Project environment ##


```{r renv_init, eval = FALSE}
renv::restore()
```

*Renv*, used above, sets up a *project environment* that stores the packages and versions used to make the project work. The idea of this is to enable you to go back to this after some time and get the project working again, no matter what's changed or been updated in the meantime. Similarly, send the project to a collaborator and they can set things up to be working quickly and easily without having to search through packages to install. I've already *initialized* this project, to get things going on your computer should a matter of opening R in this filter. *Renv* will then install itself and any packages needed into its own library.

The following packages are used:

- *CoordinateCleaner*: for checking occurrence data quality [@Zizka2019MEE] (requires [proj](https://proj.org/download.html) and [gdal](https://gdal.org/download.html)).
- *countrycode*: for cleaning occurrence data [@ArelBundock2018JOSS].
- *jsonlite*: reading JSON-structured data [@Ooms2014].
- *rnaturalearth*: country outlines for plotting maps [@South2017].
- *rnaturaleearthdata*: for checking occurrence localities [@South2017a].
- *sf*: wrangling geometry and map data [@Pebesma2018RJ].
- *tidyverse*: data wrangling pipelines [@Wickham2019JOSS].
- *viridis*: colour blind-friendly plotting [@Garnier2021].

```{r packages}
library(brms)
library(CoordinateCleaner)
library(countrycode)
library(GeoRange)
library(jsonlite)
library(rnaturalearth)
library(rnaturalearthdata)
library(sf)
library(stringdist)
library(tidyverse)
library(viridis)

options(mc.cores = parallel::detectCores())
rstan::rstan_options(auto_write = TRUE)
```

## Useful background and sites ##

I'm rather opinionated about code and formatting. I think it's generally better to be strict about the packages you use and how you write the code. I tend to use *tidyverse* style and formatting, which has [a guide](https://r4ds.had.co.nz/index.html) written by Hadley Wickham, who's created some of the most used R packages. Google have a [style guide](https://google.github.io/styleguide/Rguide.html) for writing R code that I recommend following, but as a guide can and should be changed to meet your needs.

I use *piping* a lot. This means passing the result of one function as the first argument of the next line. R has a native version shown as `%>%`, but this is rather new (version 4.1.0). *tidyverse* and package *magrittr* has had a different version for while that looks like `%>%` and is probably the better to use, so you might see that online more often.

This guide is written in [R Markdown](https://rmarkdown.rstudio.com/articles_intro.html), a text format that combines text and code and can be run easily in R Studio and other environments. I also tend to use *ggplot* for plotting, so have a look [here](https://ggplot2-book.org) for the low-down on that package.


# Acquiring data

The data come from the [Paleobiology Database](https://paleobiodb.org/#/) (PBDB). Occurrences are downloaded though their [API](https://paleobiodb.org/data1.2/).

First we can download the data as a JSON file and save this to disc: this is our master data set. For this, I've gathered occurrences of Bivalvia from the Cretaceous and Palaeogene with all of the occurrence, geographical, systematic, and ecological data. This only need be done once (and I've included the file in the repository already).

```{r download_pbdb_data, eval = FALSE}
bivalve_pbdb_url <-
  paste0(
    "https://paleobiodb.org/data1.2/", # PBDB API URL
    "occs/", # occurrences
    "list.json?", # download a JSON file
    "base_name=Bivalvia&", # download bivalve occurrences
    "interval=Cretaceous,Paleogene&", # temporal range to include
    "show=full" # include full data
  )
bivalve_pbdb_file <- paste0(Sys.Date(), "-bivalve_pbdb_occurrences.json")
download.file(bivalve_pbdb_url, bivalve_pbdb_file, method = "curl")
```

The data file can then be loaded into R, which gives a list with length two: the first element (`$elapsed_time`) gives the original download time and the second element (`$records`) is a `data.frame` of the PBDB occurrence data. We keep just this second part. To save space, I've downloaded a version and stored this as a compressed `tar` file that needs to be expanded first with `untar`.

```{r read_pbdb_data, cache = TRUE}
bivalve_pbdb_file <- "2022-03-18-bivalve_pbdb_occurrences"
untar(paste0(bivalve_pbdb_file, ".tgz"))
bivalve_pbdb_data <-
  jsonlite::fromJSON(paste0(bivalve_pbdb_file, ".json"))$records %>%
  as_tibble() %>%
  mutate(across(c(rnk, lng, lat), as.numeric))
```

We won't need data from all the columns, but the important few are:

* `oid`: occurrence ID, a unique reference number.
* `idn`: the originally identified name.
* `tna`: the currently accepted name.
* `rnk`: taxonomic rank of the accepted name.
* `oei`, `oli`: earliest and latest occurrence interval names.
  - `eag` and `lag` are ages at the beginning and end of these intervals, taken from the [*Geologic Time Scale*](https://timescalecreator.org/index/index.php), but may want to update to a recent version.
* `fml`: family of the occurring taxon.
  - higher taxon levels are also included.
* `lng`, `lat`: modern longitude and latitude of the occurrence.
  - `pln` and `pln` are the reconstructed palaeocoordinates, but it may be worth re-doing these to confirm or with a different tectonic model.
* `env`: the palaeoenvironment, useful to check localities and habitats, exclude freshwater species etc. 
* Various [palaeocological/ecospace data](https://paleobiodb.org/data1.2/general/ecotaph_doc.html#Ecospace), such as habitat depth (`jlh`), general environment (`jev`), feeding mode (`jdt`).
* `jco`: composition/mineralogy of shells.

A full list of the fields is at the bottom of [this page](https://paleobiodb.org/data1.2/occs/list_doc.html#response).

# Querying data #

It's a good check to have a quick look at the data to check its size and what different values are contained within. Most of the PBDB data are categorical or strings, which makes it a little more difficult (you can't plot a bar chart for instance, except you can). A few things to check:

* How many records are identified to species level? genus? family?
* How many different species/genera/families are include?
* How many records don't have accepted values?

```{r check_species}
bivalve_pbdb_data %>%
  filter(rnk == 3) %>%
  group_by(tna) %>%
  summarize(n())
```

Or a more fun thing to plot the occurrences from different families.

```{r plot_family_occurrences}
family_occurrences <-
  bivalve_pbdb_data %>%
    filter(rnk == c(3, 4, 5)) %>%
    group_by(fml) %>%
    summarize(family_occurrences = n())
family_occurrences %>%
  ggplot(
    aes(x = reorder(fml, desc(family_occurrences)), y = family_occurrences)
  ) +
    geom_col()
family_occurrences %>%
  slice_max(family_occurrences, n = 10)
```


# Data quality #

Now we can do a more thorough look through the data to check that it's correct, or at least in the form we expect it to be. Fortunately there is a handy package, *CoordinateCleaner*, that can help with this. It checks for coordinates that are present in the middle of countries or at (0, 0) — implying they have imprecise, or generic location data — or those occurrences that are spatiotemporal outliers — indicating they may not be correct identifications. Q guide through these checks is [here](https://docs.ropensci.org/CoordinateCleaner/articles/Cleaning_PBDB_fossils_with_CoordinateCleaner.html).

### Spatial checks ###

This first step checks that the coordinates are included as numbers and that there aren't any potential errors — such as matching latitude and longitude values. It's recommended to also check which record, if any, are flagged by adding `value = "flagged"` to all commands.

```{r check_occ_coordinate_validity}
occ_checking <-
  bivalve_pbdb_data %>%
  cc_val(lat = "lat", lon = "lng") %>%
  cc_equ(lat = "lat", lon = "lng")
```

Next we check for the location proximity to country centres, indicating imprecision.

```{r check_occ_coordinate_precision}
occ_precision <-
  occ_checking %>%
  cc_cen(lat = "lat", lon = "lng", value = "flagged")

occ_precision_fl <-
  bivalve_pbdb_data %>%
  filter(!occ_precision)
```

Then we check whether occurrences are located within the country they are meant to come from. This requires using ISO 3-letter country codes, while the PBDB uses 2-letter codes, so some conversion is needed; it also uses UK while the country codes is GB and GBR.

```{r check_occ_country_location}
# Add record for UK -> GBR, AA -> ATA [Antarctica]
cs_ma <- c("GBR", "ATA")
names(cs_ma) <- c("UK", "AA")
occ_checking <-
  occ_checking %>%
  mutate(
    cc_iso3 = countrycode(
        cc2,
        origin = "iso2c",
        destination = "iso3c",
        custom_match = cs_ma
      )
  )

occ_country <-
  occ_checking %>%
  cc_coun(lat = "lat", lon = "lng", iso3 = "cc_iso3", value = "flagged")

occ_country_fl <-
  bivalve_pbdb_data %>%
  filter(!occ_country)
```

Fourth, is to check against host institutions on the Global Biodiversity Interchange Facility, in case the locations are listed as that rather than the in-the-field locality.

```{r check_occ_gbif_institute}
occ_inst <-
  occ_checking %>%
  cc_inst(lat = "lat", lon = "lng", value = "flagged")

occ_gbif <-
  occ_checking %>%
  cc_gbif(lat = "lat", lon = "lng", value = "flagged")
```

And finally, check for records at (0, 0).

```{r check_occ_zero_zero}
occ_zero_zero <-
  occ_checking %>%
  cc_zero(lat = "lat", lon = "lng", value = "flagged")
```

### Temporal checks ###

Alongside check that occurrences are not at potentially imprecise/incorrect localities, also see whether any are erroneous separated in time is useful. First removing occurrences without ages.

```{r check_occ_empty_ages}
occ_empty_ages <-
  occ_checking %>%
    filter(
      !is.na(lag),
      !is.na(eag)
    ) %>%
  cf_equal(min_age = "lag", max_age = "eag", value = "flagged")
```

Next, check on the precision of dating as the very imprecisely dated occurrences can be readily excluded.

```{r check_occ_dating_precision, fig.cap = "Histogram of Bivalvia occurrence dating ranges from the Paleobiology Database."}
bivalve_pbdb_data %>%
  mutate(dating_precision = eag - lag) %>%
  ggplot(aes(dating_precision)) +
    geom_histogram()
```

We're mostly doing okay, with precision less than 20 Ma, but some extend all the way up to 140 Ma dates; we don't want those!*CoordinateCleaner* also provides a way to clean based on range comparisons between taxa and comparing similarities — excluding occurrences based on the range of differences of dating precision rather than just setting an arbitrary cutoff. But an absolute value can sometimes be useful: in this case using 35 Ma as the length of the Late Cretaceous. If occurrences can't certainly be dated within that time frame than we can readily exclude them. As a check of how removing these occurrences affects the spread of ranges, we plot the histogram again.

```{r check_occ_exclude_dates, fig.cap = "Histogram of cleaned occurrence dating ranges."}
occ_ranges_total <-
  occ_checking %>%
  cf_range(taxon = "", min_age = "lag", max_age = "eag", value = "flagged")

occ_ranges_taxon <-
  occ_checking %>%
  cf_range(taxon = "tna", min_age = "lag", max_age = "eag", value = "flagged")

occ_ranges_absolute <-
  occ_checking %>%
  cf_range(
    taxon = "tna",
    min_age = "lag", max_age = "eag",
    method = "time", max_range = 35, value = "flagged"
  )

bivalve_pbdb_data %>%
  filter(occ_ranges_total & occ_ranges_taxon & occ_ranges_absolute) %>%
  mutate(dating_precision = eag - lag) %>%
  ggplot(aes(dating_precision)) +
    geom_histogram(bins = 50)
```

Most of the occurrences in this data set can be dated within 10 Ma, so that's good.

Now for checking outliers in time and space — those that are much separated from their main cluster and so may be a misidentification.

```{r check_occ_outliers}
# this function exhausted memory, so I don't run it
# occ_outliers_total <-
#   occ_checking %>%
#   cf_outl(
      # taxon = "",
      # lat = "lat", lon = "lng",
      # min_age = "lag", max_age = "eag",
      # value = "flagged"
    # )

occ_outliers_taxon <-
  occ_checking %>%
  cf_outl(
    taxon = "tna",
    lat = "lat", lon = "lng",
    min_age = "lag", max_age = "eag",
    value = "flagged"
  )
```

## Excluding questionable data

The above functions let you go through and check the removed occurrences for the reasons and to make sure they should be removed. This code below does all the cleaning automatically using the helper function `clean_fossils`.

```{r occ_full_cleaning, cache = TRUE}
cs_ma <- c(
  "UK" = "GBR",
  "AA" = "ATA"
)
bivalve_pbdb_data <-
  bivalve_pbdb_data %>%
  mutate(
    cc_iso3 = countrycode(cc2,
      origin = "iso2c", destination = "iso3c", custom_match = cs_ma
    )
  )

bivalve_cleaned <-
  bivalve_pbdb_data %>%
  clean_fossils(
    taxon = "tna",
    min_age = "lag", max_age = "eag",
    lon = "lng", lat = "lat",
    countries = "cc_iso3", value = "clean"
  ) %>%
  filter(str_detect(jev, "marine|shelf|oceanic"))
```

That last line filters only those occurrences from marine/shelf/oceanic environments, based on the lithology. Have a look through the column and see whether you want to include any other environments; separate them with the vertical bar — `|`, meaning 'or' in regular expressions. The `clean_fossils` function outputs useful messages to show what stage it's at; I get some warnings about decimallatitude and decimallongitude, but I don't think that's anything to worry about.

## Checking taxonomy

A final set of things to check is for names that might have misspellings or missing taxonomy data. 

### Remove subgenus names ###

One of the features of many names is including a subgenus name in parentheses. While taxonomically important, this can be problematic as some taxa will include the subgenus, while others may not depending on the taxonomy. We thus remove the subgenus name. At this point, it's also useful to convert the strings to lowercase to remove any errors from capitalization. Searching and modifying strings in R uses a system 'regular expressions': a series of indicators to select certain groups of characters. In the example below1, we're looking for a set of lowercase letters1, surrounded by parentheses and followed by a space. Have a look at [this cheat sheet for stringr](https://raw.githubusercontent.com/rstudio/cheatsheets/main/strings.pdf) (there are cheat sheets for some other packages to look at here: <https://www.rstudio.com/resources/cheatsheets/>).

```{r remove_subgenus}
bivalve_cleaned <-
  bivalve_cleaned %>%
  mutate(tna_trimmed = str_remove_all(str_to_lower(tna), "\\(([:lower:]+)\\)\\s*"))
```

### Check spelling differences ###

Next, we check for spelling differences. Here, we use the *distance* between strings, in essence the number of characters different, to check whether there are misspelled names. Package *stringdist* has the function `stringdistmatrix` that calculates the pairwise distances within a string vector, thus showing the most similarly-spelled taxon names. Using an arbitrary distance cutoff — in this case 4, but `string_cutoff` can be changed — will show the most similar strings.

```{r spelling_errors}
str_dist_matrix <-
  bivalve_cleaned %>%
  filter(rnk == 3) %>%
  pull(tna_trimmed) %>%
  unique() %>%
  stringdistmatrix(useNames = "strings", method = "lcs") %>%
  as.matrix()

string_cutoff <- 4

str_indices <-
  tibble(
    pos_n       = which(str_dist_matrix <= string_cutoff & str_dist_matrix > 0),
    row_n       = pos_n %/% dim(str_dist_matrix)[2],
    row_n_fixed = replace(row_n, row_n == 0, dim(str_dist_matrix)[2]) + 1, # need to add 1 to offset mod(1583) = 0
    col_n       = pos_n %%  dim(str_dist_matrix)[1],
    col_n_fixed = replace(col_n, col_n == 0, dim(str_dist_matrix)[1]),
    row         = rownames(str_dist_matrix)[row_n_fixed],
    col         = colnames(str_dist_matrix)[col_n_fixed],
    distance    = str_dist_matrix[pos_n]
  ) %>%
  arrange(distance)
write_csv(str_indices, "./output/similar_spellings.csv")
```

At this point, we check the names against accepted names in [WoRMS](https://www.marinespecies.org/aphia.php?p=search), [LifeWatch](https://www.lifewatch.be/data-services/), [GBIF](https://www.gbif.org), or *Treatise on Invertebrate Paleontology*. Then, we use a conversion vector to correct these in the data: the incorrect version is before the equals and the corrected version after. Note this output table shows differences both ways (row and column), so each minor difference is shown twice with the names switched.

```{r correct_taxon_spellings}
name_corrections <-
  c(
    "syncyclonema haggi" = "syncyclonema haeggi",
    "incorrect" = "correct"
  )

bivalve_cleaned <-
  bivalve_cleaned %>%
  mutate(tna_trimmed = str_replace_all(tna_trimmed, name_corrections))
```

We now use `tna_trimmed` as the main taxon name-column.

### Check family name validity ###

We can check what families are in included in the data quite easily but *pulling* this column out and finding unique records. 

```{r family_records}
bivalve_cleaned %>%
  pull(fml) %>%
  unique()
```

We see that there are two errant values: "NO_FAMILY_SPECIFIED" and `NA`.

```{r show_uncertain_families}
errant_taxonomy <-
  bivalve_cleaned %>%
  filter(fml == "NO_FAMILY_SPECIFIED" | is.na(fml)) %>%
  select(tna_trimmed, fml, rnk)
print(errant_taxonomy, n = Inf)
errant_taxonomy %>%
  group_by(rnk) %>%
  summarize(n = n())

errant_taxonomy %>% pull(tna_trimmed) %>% unique()
```

This finds `r nrow(errant_taxonomy)` records that wither have no family or `NA`. Most are identified to genus level, but a similar number are species. It turns out that those with `NA` in the family are only identified to a higher taxonomic level, so we can leave them alone. 

The code below instead shows the lower taxa without a family specified. These are ripe for inclusion.

```{r taxa_no_family}
errant_families <-
  bivalve_cleaned %>%
  filter(fml == "NO_FAMILY_SPECIFIED") %>%
  select(tna_trimmed, fml, rnk)
print(errant_families, n = Inf)
```

We can write this out to a file that can then be uploaded to LifeWatch and use that to compare against its databases.

```{r write_family_lifewatch, eval = FALSE}
errant_families %>%
  select(tna_trimmed) %>%
  unique() %>%
  transmute(ScientificName = tna_trimmed) %>%
  write_csv("./output/uncertain_taxa.csv")
```

This list of uncertain taxa can then be compared against an online data base, such as [LifeWatch](https://www.lifewatch.be/data-services/).

```{r}
lifewatch_taxon_check <-
  read_csv("./output/19470_uncertain_taxa.csv")

print(lifewatch_taxon_check)
```


# Mapping data #

We can also do more fun things like plot the occurrences on a map. Useful guides to this are found in three blog posts [starting here](https://r-spatial.org//r/2018/10/25/ggplot2-sf.html). Working with mapping data is more involved than simple points, particularly if you want to change the projection away from the standard Mercator. The package *sf* (short for 'simple features', describing how the data is stored) can deal with all of this and then in plotting the projection can be changed. A rule of thumb is make sure to assign the coordinate reference system (CRS) when creating map objects.

Package `rnaturalearth` has regularly updated maps that should be better to use than in the built-in `maps` library. Here we download the country outlines in *sf* format.

```{r map_occurrences}
world_map <-
  ne_countries(returnclass = "sf")
```

Next, convert the occurrences locations into an *sf* object also. I've filtered down to occurrences identified to species level and then taken a random sample ('slice') of 1000 rows. The final stage is convert to an *sf* object using the longitude and latitude columns, with the standard WGS84 CRS (indicated with `4326` in this case). You can find this code to use looking at [epsg.io](https://epsg.io/4326), searching for the projection, and using the code given. In the case of WGS84 coordinates (standard latitude and longitude) shown this is 4326.

```{r get_occurrence_points}
occurrences_to_plot <-
  bivalve_cleaned %>%
  filter(rnk == 3) %>%
  slice_sample(n = 1000) %>%
  st_as_sf(coords = c("lng", "lat"), crs = 4326, agr = "constant")
```

Now, we put these together into the plot using the `geom_sf` function to build up the layers: map outline first, then points over the top. The colour scale is good for colour-blind people, so is a good option to prefer. Here I modify the map projection using `coord_sf(crs)` into the [Robinson projection](https://en.wikipedia.org/wiki/Robinson_projection).

The essence of plotting with `geom_sf` is to add the different layers one by one. In this case, the first call plots the country outlines then the second line plots the occurrence points over the top; `scale_colour_viridis` is a good set of plotting colours to use as they are colour blind-friendly; and the final theme options remove the border and the grey background usual in *ggplot* plots.

```{r plot_occurrence_map, fig.cap = "Map of Bivalvia localities from the Cretaceous and Palaeogene.", fig.width = 7, fig.height = 4}
ggplot() +
  geom_sf(data = world_map, inherit.aes = FALSE) +
  geom_sf(data = occurrences_to_plot, mapping = aes(colour = fml)) +
  scale_colour_viridis(discrete = TRUE) +
  coord_sf(crs = "+proj=robin") +
  theme_bw() +
  theme(
    legend.position = "none",
    panel.border = element_blank()
  )
```

## Palaeogeographical map ##

We use the palaeogeographical reconstructions from the combined Muller2019-Young2019-Cao2020 (MYC) model provided by GPlates (<https://www.gplates.org>) [@Muller2019T; @Young2019GF; @Cao2022GR; @Torsvik2019GGG]. This provides a nice visualization of the palaeogeography including locations of shallow marine environments, land, mountains, and ice caps. We exported these four layers reconstructed to 68 Ma in the latest Cretaceous and plot them as a base map with modern coastlines indicated. The data files for the reconstructions are provided with GPlates or can be downloaded from [Earthbyte](http://www.earthbyte.org/gplates-2-3-software-and-data-sets/).

The different layers (shallow marine, land, mountain, ice cap) are included in four files in the `./data/palaeogeographical_reconstructions` directory and read into a list then combined using the functions below. Combining these data sets into one makes it quicker to plot and colour the different layers in the base map.

```{r palaeogeo_base_map, fig.cap = "Palaeogeographical reconstruction in the latest Cretaceous (68 Ma), with modern coastlines indicated, based on the environmental reconstrution of @Cao2017Bb and the combined rotaion models of @Muller2019T, @Young2019GF, and @Cao2022GR, with correction to the Pacific by @Torsvik2019GGG.", fig.width = 9, fig.height = 4}
palaeogeo_file_layers <-
  c("Shallow marine" = "sm", "Landmass" = "lm", "Mountain" = "m", "Ice cap" = "i")

read_geojson <-
  function(prefix, filename = "./data/palaeogeographical_reconstructions/id_402_2_reconstructed_68.00Ma.gmt") {
  # Read a list of GeoJSON files and combine into a single sf collection.
  #
  # Arguments:
  #   prefix: layer prefixes for the data files ("sm", "lm", "m", "i") taken from Cao et al. (2017).
  #   filename: rest of GeoJSON file name.
  #
  # Returns:
  #   A single feature collection with geometries and ID column ("id").
  str_replace(filename, "id", prefix) %>%
    st_read() %>%
    add_column(id = prefix)
}

map_data <-
  palaeogeo_file_layers %>%
    purrr::map(read_geojson) %>%
    do.call(rbind, .) %>%
    mutate(
      id = factor(id, levels = palaeogeo_file_layers, labels = names(palaeogeo_file_layers))
    )

base_map <-
  ggplot() +
    geom_sf(data = map_data, aes(fill = id), colour = NA) +
    scale_discrete_manual(
      # colours for base map layers
      values =
        c(
          "Ice cap"        = "#DAD3FF",
          "Landmass"       = "#FFD23A",
          "Mountain"       = "#FF8D51",
          "Shallow marine" = "#45D8FF"
        ),
      aesthetics = c("fill"),
      name = "Palaeogeography"
    ) +
    theme_bw() +
    theme(panel.border = element_blank())

modern_coastlines <-
  st_read("./data/palaeogeographical_reconstructions/Global_EarthByte_GPlates_PresentDay_Coastlines_Polyline_reconstructed_68.00Ma.gmt")

base_map +
  geom_sf(data = modern_coastlines, colour = "grey60") +
  coord_sf(crs = "+proj=robin")
```

## Rotating modern locality coordinates

We have the modern localities for our bivalve occurrences, but need to *rotate* these to their position at the end of the Cretaceous (68 Ma) to get their palaeocoordinates. The GPlates Web Service can do this online or through docker, but doesn't offer the rotation model used for the combined MYC palaeogeographical reconstruction, and can be quite slow to do many requests — and we have tens-of-thousands of bivalve occurrences. Instead, we field this out to [pyGPlates](https://www.gplates.org/docs/pygplates/index.html). This step of reconstruction has been done, so the file `./data/reconstructed_68Ma.gmt` can be read in directly.

```{r write_bivalve_coords, eval = FALSE}
kpg_occurrences <-
  bivalve_cleaned %>%
  filter(lag >= 66 & lag < 70)

modern_coords <-
  kpg_occurrences %>%
  # select(c("lng", "lat")) %>%
  st_as_sf(coords = c("lng", "lat"), crs = 4326)

st_write(modern_coords, "./data/kpg_bivalve_modern_coordinates.gmt", driver = "OGR_GMT")
```

Instructions to install pyGPlates can be found [here](https://www.gplates.org/docs/pygplates/index.html), including the necessary Python packages (specifically `proj`). [Anaconda](https://www.anaconda.com)/[Miniconda](https://docs.conda.io/en/latest/miniconda.html) is a good Python managing system in which you can also create project environments. The code in the python blocks below can be run in a python environment — separate to the main R environment — or can be run directly within R using the *reticulate* package [@Ushey2022].

There are two steps to reconstructing the palaeocoordinates: (1) assigning the localities to the static polygons (i.e. plate fragments thay move with continental drift), and (2) rotating the plate fragments and localities to the palaeolocations. First we assign the points and output a GPML file that can be visualized in GPlates directly.

```{python assign_locality_polygons, eval = FALSE}
import pygplates

# Load one or more rotation files into a rotation model.
rotation_model = pygplates.RotationModel('./data/palaeogeographical_reconstructions/Muller2019-Young2019-Cao2020_CombinedRotations.rot')

# Filename for the static plate boundaries
static_polygons_filename = './data/palaeogeographical_reconstructions/Global_EarthByte_GPlates_PresentDay_StaticPlatePolygons.gpmlz'

# Load some features.
point_features = pygplates.FeatureCollection('./data/kpg_bivalve_modern_coordinates.gmt')

# Output filename for assigned points
assigned_points_output = './data/bivalve_assigned_locations.gpml'

# Place point occurrences onto plate static polygons
assigned_points = pygplates.partition_into_plates(
    static_polygons_filename,
    rotation_model,
    point_features,
    properties_to_copy = [
        pygplates.PartitionProperty.reconstruction_plate_id,
        pygplates.PartitionProperty.valid_time_period
    ]
)

# Write assigned points to a GPML file that can be used in GPlates
assigned_points_collection = pygplates.FeatureCollection(assigned_points)
assigned_points_collection.write(assigned_points_output)
```

Next we rotate the palaeolocations to 68 Ma and export an OGR-GMT file of these palaeocoordinates.

```{python reconstruct_palaeocoordinates, eval = FALSE}
# Reconstruct features to this geological time.
reconstruction_time = 68

# The filename of the exported reconstructed geometries.
# It's a shapefile called 'reconstructed_68Ma.shp'.
export_filename = './data/reconstructed_{0}Ma.gmt'.format(reconstruction_time)

# Reconstruct the features to the reconstruction time and export them to a shapefile.
pygplates.reconstruct(assigned_points_collection, rotation_model, export_filename, reconstruction_time)
```

These converted locations can be read back into R and plotted over the base map.

```{r read_palaeo_coords, fig.cap = "Palaeolocations of marine bivalve occurrences in the latest Cretaceous (70–66 Ma) plotted on a palaeogeographical map at 68 Ma.", fig.width = 9, fig.height = 4}
palaeo_coords <-
  st_read("./data/reconstructed_68Ma.gmt")

base_map +
  geom_sf(data = palaeo_coords) +
  coord_sf(crs = "+proj=robin")
```


# Palaeoenvironmental data

```{r}
#TODO: import netCDF data
palaeoenvironment <-
  st
```


# Finding extinctions

We find taxa that go extinct by charting those present before the K/Pg boundary that aren't present afterwards. Initially, do this for each species: we group into families later. Filtering out the species that go extinct, we choose those that have their last appearances after 70 Ma but before 66 Ma; you can be more precise and do after 68 Ma, but I don't know whether the PBDB is dated precise enough to make much difference here.

```{r species_extinctions}
species_extinctions <-
  bivalve_cleaned %>%
    filter(
      rnk == 3,
      lag >= 66 & lag <= 70
    ) %>%
    group_by(tna_trimmed) %>%
    summarize(kpg_ext = 1)
```

In the context of families, we can see the proportions of the families that go extinct. We choose species from the Maastrichtian and Dining, join on the previous table identifying those species that go extinct, then group into families and count the proportion that go extinction at the K/Pg boundary.

```{r family_extinction_proporation, fig.cap = "Proportions of bivalve families that go extinction across the Cretaceous/palaeogene boundary."}
fml_extinction_proportion <-
  bivalve_cleaned %>%
  filter(
    rnk == 3,
    lag >= 61.2 & lag <= 72 & eag >= 66
  ) %>%
  left_join(species_extinctions, by = "tna_trimmed") %>%
  select(tna_trimmed, fml, kpg_ext) %>%
  distinct(.keep_all = TRUE) %>%
  mutate(kpg_ext = replace(kpg_ext, is.na(kpg_ext), 0)) %>%
  group_by(fml, kpg_ext) %>%
  summarize(n = n()) %>%
  mutate(ext_prop = n / sum(n)) %>%
  filter(kpg_ext == 1) # select the extinction proportion

fml_extinction_proportion %>%
  ggplot(aes(x = fml, y = ext_prop)) +
  geom_col() +
  coord_flip()
```

This last plot may also benefit from showing the number of species for each family (to gauge the relative magnitude) and another version showing the absolute numbers.


# Starting modelling

These last chunks essentially bring together most of the things we need to start modelling. These first few bits below will start with showing the general workflow for modelling extinctions across the Cretaceous–Palaeogene boundary and can then be readily extended to ask other or more specific questions. There are two levels to look at when modelling the extinction of these Bivalve taxa: (1) the family level, and (2) the species level.

Family extinctions
: Using the proportion of the family that goes extinct as the variable to predict.

Species extinctions
: Predicting the extinction of species as an absolute: whether it does or does not go extinct.

You'll be familiar with linear modelling, typified by the equation \(y = mx + c\). In our case, \(y\) is extinction — the value we're trying to predict — while \(x\) is the predictor that we think has an effect, or drives extinction — temperature, area, salinity etc. In other language, extinction is the dependent variable while temperature, area, salinity are the independent variables. Additional complexity comes in because standard linear modelling assumes that the dependent variable is normally-distributed. What \(y = mx + c\) is shorthand for is this:

\[
  y ~ \textrm{Normal} \left( \mu, \sigma \right) \\
  \mu = mx + c,
\]

where \(\mu\) is the mean and \(\sigma\) is a constant standard deviation. You're predictors, \(x\) are use to work out the mean of a normal distribution, \(\mu\), which has a fixed standard deviation, \(\sigma\). Going through the normal distribution is where the 'randomness' inherent in the data comes from and accounts for the spread of the data about the best-fit line. Using this normal distribution model is fine for heights or lengths or other things like that, but we have two different examples:

1. Family extinctions is a proportion, so must between 0–1. You could force a very narrow normal distribution with a small standard deviation, but the maths extends to infinity no matter how unlikely the probably may be.
2. Species extinctions will be a yes/no on whether that species goes extinct (included as 1 or 0). The normal distribution is continuous, so cannot easily be applied to discrete values like this.

This is where the extension of *generalized linear modelling* comes in: this relaxes the requirement to use a normal distribution for the dependent variable, so now we can predict proportions or discrete values. 

## Family extinctions ##

Family extinctions we've calculated as the proportion of the family that goes extinction (between 0–1). Therefore, to model this we need to use a distribution that extends between 0–1, but not beyond that. This is prime territory for the [*beta distribution*](https://en.wikipedia.org/wiki/Beta_distribution), so our model instead looks like this:

\[
  E_i ~ \textrm{Beta} \left( \bar{p}_i, \theta \right) \\
  \textrm{logit} \left( \bar{p}_i \right) = m_x x_i + c \\
  m_x ~ \textrm{Normal} \left( 0, 10 \right) \\
  c ~ \textrm{Normal} \left( 0, 10 \right) \\
  \theta = \phi + 2 \\
  \phi ~ \textrm{Normal} \left( 0, 10 \right),
\]

where \(E\) is the proportion of the family that goes extinct, \(\bar{p}\) is the Beta distribution mean and \(\theta\) its shape parameter, \(x\) is the predictor, \(m\) and \(c\) are the coefficient and intercept of the linear model. For the Beta distribution, \(\theta\) determines the shape: whether the probabilities are condensed (e.g. 0.4–0.6) into the middle or focused around the edges (e.g. 0 or 1) with \(\theta = 2\) giving a flat distribution. Here we add the extra variable \(\phi\) so that \(\theta\) is centred around 2. The logit expression is needed to convert the ingoing model into a value between 0–1 as is required for \(\bar{p}\).

The additional feature here is that this is a more *Bayesian* realization of generalized linear modelling: the parameters in the model (\(\bar{p}, m_x, c, and \phi\)) all have probability distributions associated with them. In essence, rather that heading towards definite values, this model and these distributions incorporate uncertainty in measurements and models so that we end up with *probabilities* for a variety of different results, with the 'best' linear model having the highest probability. The model can't tell you the exact level of extinction that a certain driver might generate (like with a very simplified linear model might suggest), but will give you the probability of different amounts of extinction, with one level hopefully being the most probable.

Most of the modelling can be done in R, but often is 'slow': adding complicated probability distributions is difficult, especially as you add more data and more complex models. This is added to R repeatedly having to keep checking between the data, working out what calculations it needs to do to get to the model values, then tell the computer to do those things. Instead we'll use the package *brms* that uses an external language called *Stan*. Stan is designed specifically for generalized linear modelling and is also compiled: this means that it starts by taking the model and making a little program that can then skip past all the copying and checking R does and goes straight the computer and calculates. Fast.

The other feature of Stan, and one that's often used when Bayesian inference is involved, is rather than sampling and calculating the probabilities directly, which is difficult, is instead to push some values through and calculate a result, then shift the parameters and calculate it again. By doing this repeatedly, you can then build up a picture of what the final distribution looks like without having to calculate it in full, complicated detail. This is called *Monte Carlo* sampling (MC) and often has some extra details called *Metropolis Coupled Monte Carlo* (MCMC).

brms is an R package that creates the Stan code from the data and model and tells Stan to run an MCMC analysis then parses the results ready to be plotted. It installs all the bits needed, so we don't need to leave R to do these analyses.

### Spread of occurrences ###

This first model will use the family proportions of extinction and will model this related to the amount of geographical space that the family encompasses. This space will be calculated using a *minimum spanning tree* (MST). An MST plots the shortest branching path between all the points included in a set of data: the more spread out the points, the longer the MST length. `MSTDist` in the chunk below calculates the MST for each family at the K/Pg boundary1, and the results are joined to the proportions calculated earlier.

```{r minimum_spanning_tree}
family_mst_data <-
  palaeo_coords %>%
  add_column(
    plng = st_coordinates(palaeo_coords)[, 1],
    plat = st_coordinates(palaeo_coords)[, 2]
  ) %>%
  st_drop_geometry() %>%
  group_by(fml) %>%
  summarize(
    n_occs = n(),
    mst = MSTDist(plng, plat)$MST_km
  ) %>%
  select(fml, n_occs, mst) %>%
  left_join(fml_extinction_proportion, by = "fml")

family_mst_data %>%
  ggplot(aes(x = fml, y = mst)) +
  geom_col() +
  coord_flip()

family_mst_data %>%
  ggplot(aes(x = mst, y = ext_prop)) +
  geom_point()
```

This last plot is a good one to check as it's the linear model we want to test: extinction against MST length. There's hints of a positive relationship but the data are certainly well spread.

A few rows have `NA` for their extinction values; these are the families that don't appear until after the K/Pg boundary. We'll remove these in a moment, but we're nearly ready to go in into modelling. The last bit will be to standardize the MST values: change all the values so that they have a mean 0 and standard deviation of 1. This reduces the effects of scale when doing the modelling; we will convert back afterwards when we need to.

```{r standardize_mst}
mst_scaling <-
  scale(family_mst_data$mst)

family_mst_scaled <-
  family_mst_data %>%
  mutate(mst_stand = mst_scaling[, 1]) %>%
  filter(!is.na(ext_prop)) %>%
  select(fml, mst_stand, ext_prop)
```

Now we generate the linear model. This takes the form above, but I'll replace a few of the parameter symbols to match that we're using MST:

\[
  E_i ~ \textrm{ZOIBeta} \left( \bar{p}_i, \phi \right) \\
  \textrm{logit} \left( \par{p}_i \right) = \alpha + \beta_M M_i \\
  \beta_M ~ \textrm{Normal} \left(0, 10 \right) \\
  \alpha ~ \textrm{Normal} \left( 0, 10 \right) \\
  \phi ~ \textrm{Gamma} \left( 0.01, 0.01 \right),
\]

where \(E_i\) is the proportion of the family that goes extinct, \(M_i\) is the scaled minimum spanning tree for the whole family, \(\alpha\) is the intercept, and \(\beta_M\) is the coefficient for \(M_i\) in the linear model. Here we use a *Zero-One-Inflated Beta distribution* (ZOIBeta) as the data has many proportions of 1.0 for our extinctions; these are likely a combination of total extinction and small numbers of species.

We now implement this in *brms*. This borrows the formula notation used in base R linear models, but adds in the extras needed for generalized linear modelling. The first thing that's useful to do is to show what parameters need to be defined initially with `get_priors`.

```{r brms_mst_priors}
get_prior(
  ext_prop ~ 0 + mst_stand,
  data = family_mst_scaled,
  family = zero_one_inflated_beta()
)
```

Here we see how the model is defined with `ext_prop ~ mst_stand`, i.e. the extinction proportion is related to the standardized MST length. Then we specify the data and the *family* or distribution to be used in the model, in this case the Beta distribution. This also includes the logit function needed. You can see all the distributions and several useful examples for *brms* in the vignettes and its [website](https://paul-buerkner.github.io/brms/index.html). A list of the available families is [here](https://paul-buerkner.github.io/brms/reference/brmsfamily.html) or in R help (`?brmsfamily`).

*brms* is clever in setting out all the bits of the model we need and providing sensible defaults. We can change these *prior distributions* (i.e. those we start the model with) in the main function using `prior` as you can see below. Running the model itself uses the same form in the function `brm`: call the formula, data, and family, then set any priors. Below we also setup `brm` to sample from the prior, so we can see the before and after, and then some housekeeping on how many jumps we want the MCMC analysis to do. More jumps samples better but takes more time, and the algorithm is clever so that it learns as it goes; thus after a while adding more iterations (`iter`) doesn't change the results and are wasted. Getting there quicker is helped by having more *chains*, which are essentially repeats of the analysis that can be combined, and using more *cores* on your computer so that you do several of these repeated analyses at the same time.

Run this code block and then we'll see what the output looks like.

```{r brms_mst_formula}
model_family_mst <-
  brm(
    ext_prop ~ mst_stand,
    data = family_mst_scaled,
    family = zero_one_inflated_beta(),
    prior = prior(normal(0, 10), coef = "mst_stand", class = "b") +
      prior(normal(0, 10), class = "Intercept"),
    sample_prior = TRUE,
    iter = 4000, chains = 4, cores = 4,
    save_pars = save_pars()
  )
```

Once the `brm` function is run, the Stan program is compiled and then the analysis proceeds. If you're running the same analysis again then it shouldn't require recompiling and goes straight to the analysis.

```{r brms_mst_summary}
summary(model_family_mst)
plot(model_family_mst)
pp_check(model_family_mst, ndraws = 100)
```


# References

